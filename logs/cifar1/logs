/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(1, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 21292
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 21364
Number of parameters: 21364
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        print(shape)
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        print(shape)
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        print(shape)
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 21364
Number of parameters: 21364
Number of parameters: 21364
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 1897168
Number of parameters: 1897168
Number of parameters: 1897168
Number of parameters: 1897168
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(90000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(90000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(90000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(90000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(90000))] 
    fc_layers = [nn.Linear(90000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=90000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=90000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=90000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=90000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=90000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=90000, bias=True)
    )
  )
  (8): Linear(in_features=90000, out_features=10, bias=True)
)
Number of parameters: 2790408
Number of parameters: 2790408
Number of parameters: 2790408
Number of parameters: 2790408
Number of parameters: 2790408
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(9000))] 
    fc_layers = [nn.Linear(9000, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=9000, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=9000, bias=True)
    )
  )
  (8): Linear(in_features=9000, out_features=10, bias=True)
)
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
Number of parameters: 279408
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,10)
        self.fc2 = nn.Linear(10, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(86528))] 
    fc_layers = [nn.Linear(86528, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=86528, out_features=10, bias=True)
      (fc2): Linear(in_features=10, out_features=86528, bias=True)
    )
  )
  (8): Linear(in_features=86528, out_features=10, bias=True)
)
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
Number of parameters: 2682776
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(5408))] 
    fc_layers = [nn.Linear(5408, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=5408, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=5408, bias=True)
    )
  )
  (8): Linear(in_features=5408, out_features=10, bias=True)
)
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
Number of parameters: 232958
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print(x.size())
        out = self.fc1(x)
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print(x.size())
        out = self.fc1(x)
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        print('11111111111111111111')
        print(x.size())
        out = self.fc1(x)
        print('222222222222222222')
        print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(676))] 
    fc_layers = [nn.Linear(676, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=676, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=676, bias=True)
    )
  )
  (8): Linear(in_features=676, out_features=10, bias=True)
)
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
Number of parameters: 29482
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 & train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='./cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Number of parameters: 39114
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0000 | Time 0.181 (0.181) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.0983 | Test Acc 0.0984
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0001 | Time 0.179 (0.079) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.3487 | Test Acc 0.3512
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0002 | Time 0.207 (0.086) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4147 | Test Acc 0.4045
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0003 | Time 0.179 (0.078) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4460 | Test Acc 0.4375
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0004 | Time 0.203 (0.098) | NFE-F 18.5 | NFE-B 0.0 | Train Acc 0.4890 | Test Acc 0.4758
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0005 | Time 0.200 (0.104) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5079 | Test Acc 0.4925
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0006 | Time 0.205 (0.099) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5108 | Test Acc 0.4908
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0007 | Time 0.200 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5266 | Test Acc 0.5035
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0008 | Time 0.210 (0.098) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.5353 | Test Acc 0.5098
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0009 | Time 0.205 (0.099) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.5365 | Test Acc 0.5088
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0010 | Time 0.254 (0.117) | NFE-F 25.0 | NFE-B 0.0 | Train Acc 0.5427 | Test Acc 0.5084
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0011 | Time 0.269 (0.125) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5388 | Test Acc 0.5045
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0012 | Time 0.583 (0.231) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5419 | Test Acc 0.5107
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0013 | Time 0.694 (0.293) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5557 | Test Acc 0.5195
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0014 | Time 0.263 (0.183) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5157
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0015 | Time 0.629 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5574 | Test Acc 0.5179
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0016 | Time 0.638 (0.285) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5573 | Test Acc 0.5120
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0017 | Time 0.544 (0.256) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5516 | Test Acc 0.5116
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0018 | Time 0.503 (0.280) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5612 | Test Acc 0.5147
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0019 | Time 0.505 (0.297) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5700 | Test Acc 0.5184
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0020 | Time 0.679 (0.290) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5659 | Test Acc 0.5201
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0021 | Time 0.683 (0.282) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5621 | Test Acc 0.5135
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0022 | Time 0.624 (0.288) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5763 | Test Acc 0.5274
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0023 | Time 0.380 (0.276) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5627 | Test Acc 0.5191
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0024 | Time 0.612 (0.287) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5716 | Test Acc 0.5244
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0025 | Time 0.595 (0.284) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5800 | Test Acc 0.5237
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0026 | Time 0.486 (0.264) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5650 | Test Acc 0.5165
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0027 | Time 0.621 (0.294) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5770 | Test Acc 0.5269
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0028 | Time 0.513 (0.291) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.5736 | Test Acc 0.5238
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0029 | Time 0.583 (0.294) | NFE-F 27.1 | NFE-B 0.0 | Train Acc 0.5764 | Test Acc 0.5234
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0030 | Time 0.882 (0.349) | NFE-F 30.2 | NFE-B 0.0 | Train Acc 0.5751 | Test Acc 0.5176
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0031 | Time 0.594 (0.313) | NFE-F 31.5 | NFE-B 0.0 | Train Acc 0.5831 | Test Acc 0.5252
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0032 | Time 0.618 (0.323) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5806 | Test Acc 0.5231
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0033 | Time 0.484 (0.353) | NFE-F 31.8 | NFE-B 0.0 | Train Acc 0.5822 | Test Acc 0.5260
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0034 | Time 0.653 (0.333) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5835 | Test Acc 0.5282
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0035 | Time 0.690 (0.334) | NFE-F 32.3 | NFE-B 0.0 | Train Acc 0.5857 | Test Acc 0.5306
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0036 | Time 0.661 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5782 | Test Acc 0.5211
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0037 | Time 0.757 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5756 | Test Acc 0.5192
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0038 | Time 0.602 (0.379) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5826 | Test Acc 0.5241
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0039 | Time 0.393 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5809 | Test Acc 0.5234
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0040 | Time 0.816 (0.376) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5859 | Test Acc 0.5260
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0041 | Time 0.579 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5873 | Test Acc 0.5298
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0042 | Time 0.607 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5851 | Test Acc 0.5286
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0043 | Time 0.476 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5519 | Test Acc 0.5031
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0044 | Time 0.759 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5850 | Test Acc 0.5279
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0045 | Time 0.554 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5225
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0046 | Time 0.508 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5855 | Test Acc 0.5289
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0047 | Time 0.530 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5907 | Test Acc 0.5277
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0048 | Time 0.670 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5869 | Test Acc 0.5284
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0049 | Time 0.755 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5847 | Test Acc 0.5247
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0050 | Time 0.493 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5892 | Test Acc 0.5297
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0051 | Time 0.543 (0.335) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5856 | Test Acc 0.5276
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0052 | Time 0.767 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5887 | Test Acc 0.5229
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0053 | Time 0.538 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5901 | Test Acc 0.5297
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0054 | Time 0.592 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5890 | Test Acc 0.5214
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0055 | Time 0.589 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5933 | Test Acc 0.5241
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0056 | Time 0.753 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5929 | Test Acc 0.5311
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0057 | Time 0.522 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5955 | Test Acc 0.5310
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0058 | Time 0.428 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6002 | Test Acc 0.5276
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0059 | Time 0.549 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5862 | Test Acc 0.5241
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0060 | Time 0.543 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5984 | Test Acc 0.5280
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0061 | Time 0.528 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6041 | Test Acc 0.5381
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0062 | Time 0.441 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5346
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0063 | Time 0.664 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5398
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0064 | Time 0.730 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6040 | Test Acc 0.5376
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0065 | Time 0.760 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6048 | Test Acc 0.5359
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0066 | Time 0.500 (0.341) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5363
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0067 | Time 0.789 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6031 | Test Acc 0.5359
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0068 | Time 0.704 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5360
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0069 | Time 0.688 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6046 | Test Acc 0.5357
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0070 | Time 0.825 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6057 | Test Acc 0.5374
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0071 | Time 0.525 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6033 | Test Acc 0.5359
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0072 | Time 0.781 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6047 | Test Acc 0.5387
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0073 | Time 0.703 (0.316) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6049 | Test Acc 0.5366
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0074 | Time 0.627 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5354
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0075 | Time 0.696 (0.351) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5365
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0076 | Time 0.633 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5369
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0077 | Time 0.587 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6054 | Test Acc 0.5342
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0078 | Time 0.663 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5353
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0079 | Time 0.620 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6064 | Test Acc 0.5348
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0080 | Time 0.606 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0081 | Time 1.013 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6055 | Test Acc 0.5357
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0082 | Time 0.690 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5361
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0083 | Time 0.678 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0084 | Time 0.766 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5358
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0085 | Time 0.841 (0.344) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0086 | Time 0.582 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5374
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0087 | Time 0.632 (0.361) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6062 | Test Acc 0.5348
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0088 | Time 0.722 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6059 | Test Acc 0.5331
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0089 | Time 0.543 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6060 | Test Acc 0.5365
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0090 | Time 0.497 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6039 | Test Acc 0.5373
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0091 | Time 0.492 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5353
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0092 | Time 0.752 (0.324) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6063 | Test Acc 0.5358
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0093 | Time 0.684 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5372
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0094 | Time 0.632 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6053 | Test Acc 0.5341
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0095 | Time 0.583 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5393
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0096 | Time 0.651 (0.354) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5370
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0097 | Time 0.509 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5366
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0098 | Time 0.455 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0099 | Time 0.667 (0.350) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6061 | Test Acc 0.5363
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0100 | Time 0.382 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6051 | Test Acc 0.5385
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0101 | Time 0.695 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5378
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0102 | Time 0.706 (0.322) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5372
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0103 | Time 0.719 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5374
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0104 | Time 0.580 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5368
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0105 | Time 0.641 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5366
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0106 | Time 0.624 (0.345) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5363
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0107 | Time 0.648 (0.321) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5366
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0108 | Time 0.517 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0109 | Time 0.549 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5372
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0110 | Time 0.617 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0111 | Time 0.841 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0112 | Time 0.594 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5365
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0113 | Time 0.443 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5370
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0114 | Time 0.622 (0.370) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6068 | Test Acc 0.5373
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0115 | Time 0.562 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5366
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0116 | Time 0.650 (0.336) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6065 | Test Acc 0.5364
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0117 | Time 0.523 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5357
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0118 | Time 0.508 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5369
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0119 | Time 0.680 (0.348) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6067 | Test Acc 0.5371
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0120 | Time 0.672 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5363
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0121 | Time 0.616 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0122 | Time 0.532 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5375
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0123 | Time 0.717 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5367
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0124 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5358
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0125 | Time 0.406 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5364
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0126 | Time 0.637 (0.318) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5373
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0127 | Time 0.612 (0.371) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0128 | Time 0.670 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5374
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0129 | Time 0.540 (0.311) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5373
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0130 | Time 0.556 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5366
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0131 | Time 0.505 (0.359) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0132 | Time 0.586 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0133 | Time 0.652 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5369
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0134 | Time 0.614 (0.369) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5357
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0135 | Time 0.594 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6066 | Test Acc 0.5365
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0136 | Time 1.115 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6070 | Test Acc 0.5367
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0137 | Time 0.671 (0.331) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6069 | Test Acc 0.5370
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0138 | Time 0.542 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5381
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0139 | Time 0.456 (0.337) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6077 | Test Acc 0.5359
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0140 | Time 0.656 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5368
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0141 | Time 0.865 (0.366) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6076 | Test Acc 0.5365
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0142 | Time 0.464 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0143 | Time 0.761 (0.329) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5360
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0144 | Time 0.685 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0145 | Time 0.663 (0.363) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5360
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0146 | Time 0.504 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0147 | Time 0.696 (0.342) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0148 | Time 0.525 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0149 | Time 0.890 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0150 | Time 0.662 (0.323) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0151 | Time 0.566 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0152 | Time 0.630 (0.373) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0153 | Time 0.595 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5364
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0154 | Time 0.707 (0.315) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0155 | Time 0.700 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0156 | Time 0.457 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5366
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0157 | Time 0.751 (0.325) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5361
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0158 | Time 0.599 (0.340) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6071 | Test Acc 0.5362
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0159 | Time 0.656 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0160 | Time 0.522 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0161 | Time 0.632 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0162 | Time 0.808 (0.357) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5362
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0163 | Time 0.618 (0.353) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0164 | Time 0.748 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0165 | Time 0.521 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0166 | Time 0.683 (0.352) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5362
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0167 | Time 0.721 (0.327) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0168 | Time 0.535 (0.313) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0169 | Time 0.548 (0.346) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0170 | Time 0.645 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6072 | Test Acc 0.5363
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0171 | Time 0.553 (0.333) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5366
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0172 | Time 0.668 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0173 | Time 0.520 (0.368) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0174 | Time 0.672 (0.326) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0175 | Time 0.631 (0.338) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0176 | Time 0.736 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0177 | Time 0.728 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0178 | Time 0.661 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0179 | Time 0.526 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5365
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0180 | Time 0.746 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0181 | Time 0.570 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5364
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0182 | Time 0.661 (0.343) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0183 | Time 0.577 (0.347) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0184 | Time 0.658 (0.374) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0185 | Time 0.518 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0186 | Time 0.730 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0187 | Time 0.750 (0.364) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6075 | Test Acc 0.5362
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0188 | Time 0.541 (0.330) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5363
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0189 | Time 0.774 (0.320) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0190 | Time 0.886 (0.317) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5364
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0191 | Time 0.826 (0.385) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5367
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0192 | Time 0.590 (0.339) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5363
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0193 | Time 0.757 (0.328) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0194 | Time 0.609 (0.360) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0195 | Time 0.508 (0.332) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0196 | Time 0.589 (0.334) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5359
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0197 | Time 0.493 (0.314) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5362
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0198 | Time 0.577 (0.362) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6073 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
Epoch 0199 | Time 0.687 (0.319) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6074 | Test Acc 0.5361
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
Number of parameters: 76526
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Number of parameters: 100606
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='../logs/cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example_tiny.py
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example_tiny.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  4 09:04:31 2021

@author: manzand
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=200)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifar1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        # print('11111111111111111111')
        # print(x.size())
        out = self.fc1(x)
        # print('222222222222222222')
        # print(out.size())
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        xout = x.view(-1, shape)
        # print(xout.size())
        return xout


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 4, 3, 1),
        norm(4),
        nn.ReLU(inplace=True),
        nn.Conv2d(4, 4, 4, 2, 1),
        norm(4),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(900))] 
    fc_layers = [nn.Linear(900, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.98 and train_acc > 0.98:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_tiny2.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='../logs/cifar1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=200, network='odenet', save='../logs/cifar1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(4, eps=4, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=900, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=900, bias=True)
    )
  )
  (8): Linear(in_features=900, out_features=10, bias=True)
)
Number of parameters: 39114
Number of parameters: 39114
Epoch 0000 | Time 0.295 (0.295) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.295 (0.295) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.203 (0.100) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.2991 | Test Acc 0.3047
Epoch 0001 | Time 0.203 (0.100) | NFE-F 14.4 | NFE-B 0.0 | Train Acc 0.2991 | Test Acc 0.3047
Epoch 0002 | Time 0.198 (0.098) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4173 | Test Acc 0.4174
Epoch 0002 | Time 0.198 (0.098) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4173 | Test Acc 0.4174
Epoch 0003 | Time 0.207 (0.092) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4515 | Test Acc 0.4447
Epoch 0003 | Time 0.207 (0.092) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.4515 | Test Acc 0.4447
Epoch 0004 | Time 0.251 (0.133) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4688 | Test Acc 0.4577
Epoch 0004 | Time 0.251 (0.133) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4688 | Test Acc 0.4577
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (11): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (11): Linear(in_features=2250, out_features=10, bias=True)
)
Number of parameters: 102236
Number of parameters: 102236
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(490))] 
    fc_layers = [nn.Linear(490, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(490))] 
    fc_layers = [nn.Linear(490, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=490, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=490, bias=True)
    )
  )
  (11): Linear(in_features=490, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=490, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=490, bias=True)
    )
  )
  (11): Linear(in_features=490, out_features=10, bias=True)
)
Number of parameters: 26556
Number of parameters: 26556
Epoch 0000 | Time 0.217 (0.217) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.217 (0.217) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.253 (0.145) | NFE-F 32.6 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.253 (0.145) | NFE-F 32.6 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0002 | Time 0.283 (0.153) | NFE-F 33.1 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0002 | Time 0.283 (0.153) | NFE-F 33.1 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0003 | Time 0.286 (0.147) | NFE-F 32.9 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0003 | Time 0.286 (0.147) | NFE-F 32.9 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0004 | Time 0.294 (0.151) | NFE-F 32.7 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0004 | Time 0.294 (0.151) | NFE-F 32.7 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0005 | Time 0.295 (0.148) | NFE-F 32.6 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0005 | Time 0.295 (0.148) | NFE-F 32.6 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/cifar/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(490))] 
    fc_layers = [nn.Linear(490, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='../logs/cifarmid')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        nn.Conv2d(10, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(490))] 
    fc_layers = [nn.Linear(490, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=100, network='odenet', save='../logs/cifarmid', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=490, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=490, bias=True)
    )
  )
  (11): Linear(in_features=490, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (8): ReLU(inplace=True)
  (9): Flatten()
  (10): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=490, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=490, bias=True)
    )
  )
  (11): Linear(in_features=490, out_features=10, bias=True)
)
Number of parameters: 26556
Number of parameters: 26556
Epoch 0000 | Time 0.254 (0.254) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.1032 | Test Acc 0.1021
Epoch 0000 | Time 0.254 (0.254) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.1032 | Test Acc 0.1021
Epoch 0001 | Time 0.278 (0.150) | NFE-F 32.1 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.278 (0.150) | NFE-F 32.1 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0002 | Time 0.271 (0.149) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0002 | Time 0.271 (0.149) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
