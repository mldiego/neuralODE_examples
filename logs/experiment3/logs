/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(1690))] 
    fc_layers = [nn.Linear(1690, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=1690, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=1690, bias=True)
    )
  )
  (8): Linear(in_features=1690, out_features=10, bias=True)
)
Number of parameters: 76526
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
/home/manzand/Documents/Python/neuralODE_examples/odenet_cifar_example.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 19 15:10:31 2021

@author: manzand
"""

import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from scipy.io import savemat

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=100)
parser.add_argument('--data_aug', type=eval, default=False, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)

parser.add_argument('--save', type=str, default='./experiment3')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


def norm(dim):
    return nn.BatchNorm2d(min(64, dim), dim)


class ConcatConv2d(nn.Module):

    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):
        super(ConcatConv2d, self).__init__()
        module = nn.ConvTranspose2d if transpose else nn.Conv2d
        self._layer = module(
            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,
            bias=bias
        )

    def forward(self, t, x):
        tt = torch.ones_like(x[:, :1, :, :]) * t
        ttx = torch.cat([tt, x], 1)
        return self._layer(ttx)


class ODEfunc(nn.Module):

    def __init__(self,dim):
        super(ODEfunc, self).__init__()
        self.fc1 = nn.Linear(dim,16)
        self.fc2 = nn.Linear(16, dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.fc2(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()

    def forward(self, x):
        self.integration_time = self.integration_time.type_as(x)
        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)
        return out[1]

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        shape = torch.prod(torch.tensor(x.shape[1:])).item()
        return x.view(-1, shape)


class RunningAverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, momentum=0.99):
        self.momentum = momentum
        self.reset()

    def reset(self):
        self.val = None
        self.avg = 0

    def update(self, val):
        if self.val is None:
            self.avg = val
        else:
            self.avg = self.avg * self.momentum + val * (1 - self.momentum)
        self.val = val


def get_cifar_loaders(data_aug=False, batch_size=32, test_batch_size=1000, perc=1.0):
    if data_aug:
        transform_train = transforms.Compose([
            transforms.RandomCrop(28, padding=4),
            transforms.ToTensor(),
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
    ])

    train_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_train), batch_size=batch_size,
        shuffle=True, num_workers=2, drop_last=True
    )

    train_eval_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=True, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    test_loader = DataLoader(
        datasets.CIFAR10(root='.data/cifar10', train=False, download=True, transform=transform_test),
        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True
    )

    return train_loader, test_loader, train_eval_loader


def inf_generator(iterable):
    """Allows training with DataLoaders in a single infinite loop:
        for i, (x, y) in enumerate(inf_generator(train_loader)):
    """
    iterator = iterable.__iter__()
    while True:
        try:
            yield iterator.__next__()
        except StopIteration:
            iterator = iterable.__iter__()


def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):
    initial_learning_rate = args.lr * batch_size / batch_denom

    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]
    vals = [initial_learning_rate * decay for decay in decay_rates]

    def learning_rate_fn(itr):
        lt = [itr < b for b in boundaries] + [True]
        i = np.argmax(lt)
        return vals[i]

    return learning_rate_fn


def one_hot(x, K):
    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)


def accuracy(model, dataset_loader):
    total_correct = 0
    for x, y in dataset_loader:
        x = x.to(device)
        y = one_hot(np.array(y.numpy()), 10)

        target_class = np.argmax(y, axis=1)
        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)
        total_correct += np.sum(predicted_class == target_class)
    return total_correct / len(dataset_loader.dataset)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def makedirs(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'

    downsampling_layers = [
        nn.Conv2d(3, 20, 3, 1),
        norm(20),
        nn.ReLU(inplace=True),
        nn.Conv2d(20, 10, 4, 2, 1),
        norm(10),
        nn.ReLU(inplace=True),
        Flatten(),
    ]


    feature_layers = [ODEBlock(ODEfunc(2250))] 
    fc_layers = [nn.Linear(2250, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    train_loader, test_loader, train_eval_loader = get_cifar_loaders(
        args.data_aug, args.batch_size, args.test_batch_size
    )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader)
                val_acc = accuracy(model, test_loader)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                if best_acc > 0.85 and train_acc > 0.85:
                    break
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
    names = []
    params = []
    for name,param in model.named_parameters():
        names.append(name)
        params.append(param.detach().numpy())
    nn1 = dict({'Wb':params,'names':names})
    for name,param in model.named_buffers():
        names.append(name)
        params.append(param.detach().numpy())
    savemat("odecnn_cifar_mid.mat",nn1)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=False, debug=False, downsampling_method='conv', gpu=0, lr=0.01, nepochs=100, network='odenet', save='./experiment3', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(20, eps=20, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(20, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): BatchNorm2d(10, eps=10, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
  (6): Flatten()
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (fc1): Linear(in_features=2250, out_features=16, bias=True)
      (fc2): Linear(in_features=16, out_features=2250, bias=True)
    )
  )
  (8): Linear(in_features=2250, out_features=10, bias=True)
)
Number of parameters: 100606
Number of parameters: 100606
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0000 | Time 0.829 (0.829) | NFE-F 20.0 | NFE-B 0.0 | Train Acc 0.1000 | Test Acc 0.1000
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0001 | Time 0.744 (0.415) | NFE-F 14.6 | NFE-B 0.0 | Train Acc 0.3059 | Test Acc 0.3099
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0002 | Time 0.832 (0.406) | NFE-F 14.2 | NFE-B 0.0 | Train Acc 0.3706 | Test Acc 0.3709
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0003 | Time 0.852 (0.437) | NFE-F 15.5 | NFE-B 0.0 | Train Acc 0.4167 | Test Acc 0.4168
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0004 | Time 0.795 (0.508) | NFE-F 19.6 | NFE-B 0.0 | Train Acc 0.4421 | Test Acc 0.4374
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0005 | Time 0.873 (0.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.4830 | Test Acc 0.4765
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0006 | Time 1.356 (0.635) | NFE-F 25.2 | NFE-B 0.0 | Train Acc 0.5163 | Test Acc 0.5028
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0007 | Time 0.931 (0.671) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5263 | Test Acc 0.5145
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0008 | Time 0.961 (0.699) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5463 | Test Acc 0.5326
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0009 | Time 1.464 (0.737) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5539 | Test Acc 0.5322
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0010 | Time 1.141 (0.668) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5730 | Test Acc 0.5453
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0011 | Time 1.065 (0.702) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5808 | Test Acc 0.5562
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0012 | Time 0.967 (0.644) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5807 | Test Acc 0.5547
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0013 | Time 0.487 (0.377) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5740 | Test Acc 0.5438
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0014 | Time 0.549 (0.323) | NFE-F 26.3 | NFE-B 0.0 | Train Acc 0.5938 | Test Acc 0.5597
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0015 | Time 0.678 (0.362) | NFE-F 26.4 | NFE-B 0.0 | Train Acc 0.6011 | Test Acc 0.5587
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0016 | Time 0.573 (0.357) | NFE-F 28.1 | NFE-B 0.0 | Train Acc 0.6135 | Test Acc 0.5715
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0017 | Time 0.581 (0.358) | NFE-F 31.9 | NFE-B 0.0 | Train Acc 0.6231 | Test Acc 0.5734
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0018 | Time 0.779 (0.436) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6279 | Test Acc 0.5712
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0019 | Time 0.559 (0.355) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6299 | Test Acc 0.5773
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0020 | Time 0.665 (0.401) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6253 | Test Acc 0.5640
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0021 | Time 1.379 (0.437) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6451 | Test Acc 0.5851
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0022 | Time 1.304 (0.678) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6501 | Test Acc 0.5817
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0023 | Time 1.320 (0.877) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6572 | Test Acc 0.5890
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0024 | Time 1.073 (0.760) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6581 | Test Acc 0.5887
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0025 | Time 1.356 (0.772) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6588 | Test Acc 0.5866
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0026 | Time 1.335 (0.740) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6675 | Test Acc 0.5949
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0027 | Time 1.074 (0.668) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6603 | Test Acc 0.5815
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0028 | Time 1.293 (0.648) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6749 | Test Acc 0.5937
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0029 | Time 1.369 (0.690) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6755 | Test Acc 0.5914
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0030 | Time 1.160 (0.750) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6822 | Test Acc 0.5911
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0031 | Time 1.676 (0.722) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6835 | Test Acc 0.5939
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0032 | Time 1.175 (0.724) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6937 | Test Acc 0.5995
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
Epoch 0033 | Time 1.154 (0.713) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6916 | Test Acc 0.5975
